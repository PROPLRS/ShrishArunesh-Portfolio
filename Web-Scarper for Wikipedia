import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Wikipedia's main page URL to extract the latest headlines or events.
URL = "https://en.wikipedia.org/wiki/Main_Page"

# Headers to mimic a browser request to avoid getting blocked.
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0",
    "Accept-Language": "en-US,en;q=0.9",
}

# Function to scrape Wikipedia's main page and extract important headlines.
def scrape_wikipedia_events(retries=3, delay=5):
    """
    Attempts to scrape Wikipedia's main page multiple times in case of connection failures.
    Extracts headlines and links from the page and returns them as a list of dictionaries.
    """
    for attempt in range(retries):
        try:
            response = requests.get(URL, headers=HEADERS, timeout=10)
            response.raise_for_status()  # Ensure the request was successful.
            soup = BeautifulSoup(response.content, "html.parser")

            events = []
            event_list = soup.select("#mp-upper ul li")  # Selects the latest news bullet points.

            if not event_list:
                print("No current events found. Website structure may have changed.")
                return []

            for event in event_list:
                title = event.get_text().strip()
                
                # Check if the <li> contains a hyperlink to the full event article.
                link_tag = event.find("a")
                if link_tag and "href" in link_tag.attrs:
                    link = link_tag["href"]
                    full_link = f"https://en.wikipedia.org{link}" if not link.startswith("http") else link
                else:
                    full_link = "No link available"

                events.append({"Title": title, "Link": full_link})

            return events
        except requests.exceptions.RequestException as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            time.sleep(delay)  # Wait before retrying to avoid spamming the server.
    
    print("Failed to retrieve current events after multiple attempts.")
    return []

# Function to save extracted events into a CSV file for later reference.
def save_to_csv(events, filename="wikipedia_events.csv"):
    """
    Saves the extracted event data to a CSV file for easy access and review.
    """
    df = pd.DataFrame(events)
    df.to_csv(filename, index=False)
    print(f"Saved {len(events)} events to {filename}")

if __name__ == "__main__":
    print("Scraping Wikipedia's main page for latest events...")
    wiki_events = scrape_wikipedia_events()
    if wiki_events:
        save_to_csv(wiki_events)
        print("Displaying the first 5 events:")
        for event in wiki_events[:5]:  # Show first 5 headlines in the console.
            print(f"{event['Title']} - {event['Link']}")
    else:
        print("No events found. Website structure may have changed or requests were blocked.")

# About the Developer:
# As someone deeply passionate about automation and data scraping, I built this script to stay updated
# with Wikipedia's latest events while improving my Python skills in web scraping and data management.
# This project is part of my journey in building real-world solutions using Python.
